{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = pd.read_csv('./train_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87540</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>497</td>\n",
       "      <td>2017-11-07 09:30:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105560</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>259</td>\n",
       "      <td>2017-11-07 13:40:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101424</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>212</td>\n",
       "      <td>2017-11-07 18:05:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94584</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "      <td>2017-11-07 04:58:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68413</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>2017-11-09 09:00:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel           click_time attributed_time  \\\n",
       "0   87540   12       1  13      497  2017-11-07 09:30:38             NaN   \n",
       "1  105560   25       1  17      259  2017-11-07 13:40:27             NaN   \n",
       "2  101424   12       1  19      212  2017-11-07 18:05:24             NaN   \n",
       "3   94584   13       1  13      477  2017-11-07 04:58:08             NaN   \n",
       "4   68413   12       1   1      178  2017-11-09 09:00:09             NaN   \n",
       "\n",
       "   is_attributed  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87540</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105560</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101424</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94584</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68413</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  os  channel\n",
       "0   87540   12  13      497\n",
       "1  105560   25  17      259\n",
       "2  101424   12  19      212\n",
       "3   94584   13  13      477\n",
       "4   68413   12   1      178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = csv_file[['ip', 'app', 'os', 'channel']]\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = csv_file[['device']].values.ravel()\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101424</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  os  channel\n",
       "2  101424   12  19      212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = csv_file.iloc[[2]][['ip', 'app', 'os', 'channel']]\n",
    "pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "result = knn.predict(pre)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.90634187]\n"
     ]
    }
   ],
   "source": [
    "print(regr.predict(pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87540</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105560</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101424</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94584</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68413</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  os  channel\n",
       "0   87540   12  13      497\n",
       "1  105560   25  17      259\n",
       "2  101424   12  19      212\n",
       "3   94584   13  13      477\n",
       "4   68413   12   1      178"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fast = X[:100]\n",
    "X_fast[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_fast = Y[:100]\n",
    "Y_fast[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_fast, Y_fast) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(svc.predict(pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which hyper-parameters are important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KNN, n_neighbors is important <br>\n",
    "In SGD, penalty and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What hyper-parameter values work best? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KNN, n_neighbors = 6<br>\n",
    "In SGD, penalty = 12, loss = \"hinge\" which gives a linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which supervised learner works best on the test data?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is the fastest and the most accurate learner. Other learner work slower than it. I even have to reduce the number of the training data to fit SVM. Linear Regression cannot predict correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "random.seed(datetime.now())\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "# Make plots larger\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 8 columns):\n",
      "ip                 200000 non-null int64\n",
      "app                200000 non-null int64\n",
      "device             200000 non-null int64\n",
      "os                 200000 non-null int64\n",
      "channel            200000 non-null int64\n",
      "click_time         200000 non-null object\n",
      "attributed_time    348 non-null object\n",
      "is_attributed      200000 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train= pd.read_csv('./train.csv') #train data subset, original too large\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83230</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:32:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17357</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:33:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35810</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:34:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45745</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>478</td>\n",
       "      <td>2017-11-06 14:34:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161007</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:35:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18787</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:36:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>103022</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:37:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>114221</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:37:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>165970</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:38:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>74544</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>459</td>\n",
       "      <td>2017-11-06 14:38:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>172522</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:38:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>105861</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:38:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>210962</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:39:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>124979</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:40:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38816</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:40:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80447</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:40:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>134575</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:43:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>57576</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:43:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7755</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:43:25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>91749</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 14:43:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ip  app  device  os  channel           click_time attributed_time  \\\n",
       "0    83230    3       1  13      379  2017-11-06 14:32:21             NaN   \n",
       "1    17357    3       1  19      379  2017-11-06 14:33:34             NaN   \n",
       "2    35810    3       1  13      379  2017-11-06 14:34:12             NaN   \n",
       "3    45745   14       1  13      478  2017-11-06 14:34:52             NaN   \n",
       "4   161007    3       1  13      379  2017-11-06 14:35:08             NaN   \n",
       "5    18787    3       1  16      379  2017-11-06 14:36:26             NaN   \n",
       "6   103022    3       1  23      379  2017-11-06 14:37:44             NaN   \n",
       "7   114221    3       1  19      379  2017-11-06 14:37:59             NaN   \n",
       "8   165970    3       1  13      379  2017-11-06 14:38:10             NaN   \n",
       "9    74544   64       1  22      459  2017-11-06 14:38:23             NaN   \n",
       "10  172522    3       1  25      379  2017-11-06 14:38:27             NaN   \n",
       "11  105861    3       1  13      379  2017-11-06 14:38:51             NaN   \n",
       "12  210962    3       1  19      379  2017-11-06 14:39:29             NaN   \n",
       "13  124979    3       1  18      379  2017-11-06 14:40:16             NaN   \n",
       "14   38816    3       1  18      379  2017-11-06 14:40:39             NaN   \n",
       "15   80447    3       1  19      379  2017-11-06 14:40:51             NaN   \n",
       "16  134575    3       1  13      379  2017-11-06 14:43:10             NaN   \n",
       "17   57576    3       1  19      379  2017-11-06 14:43:14             NaN   \n",
       "18    7755    3       1  13      379  2017-11-06 14:43:25             NaN   \n",
       "19   91749    3       1  19      379  2017-11-06 14:43:51             NaN   \n",
       "\n",
       "    is_attributed  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "5               0  \n",
       "6               0  \n",
       "7               0  \n",
       "8               0  \n",
       "9               0  \n",
       "10              0  \n",
       "11              0  \n",
       "12              0  \n",
       "13              0  \n",
       "14              0  \n",
       "15              0  \n",
       "16              0  \n",
       "17              0  \n",
       "18              0  \n",
       "19              0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199652\n",
       "1       348\n",
       "Name: is_attributed, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"is_attributed\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 8 columns):\n",
      "ip                 200000 non-null int64\n",
      "app                200000 non-null int64\n",
      "device             200000 non-null int64\n",
      "os                 200000 non-null int64\n",
      "channel            200000 non-null int64\n",
      "click_time         200000 non-null object\n",
      "attributed_time    356 non-null object\n",
      "is_attributed      200000 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 12.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_test= pd.read_csv('./train.csv', nrows=200000,skiprows=range(1, 400000)) #train data subset, original too large\n",
    "\n",
    "\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115115</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>178</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>144498</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>178</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76919</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>237</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1556</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>67467</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20266</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>459</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>31564</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>265</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1732</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>134</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>111114</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>245</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>121472</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>215</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>95559</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>125</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>111182</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>213</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>2017-11-06 17:29:34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>92431</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>319</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>68550</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>121</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38692</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>328</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>186002</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>280</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>61664</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>134</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24700</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>77943</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>265</td>\n",
       "      <td>2017-11-06 16:07:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ip  app  device  os  channel           click_time  \\\n",
       "0   115115    8       1  13      145  2017-11-06 16:07:47   \n",
       "1    21633    1       1  19      178  2017-11-06 16:07:47   \n",
       "2   144498   12       1  17      178  2017-11-06 16:07:47   \n",
       "3    76919    2       1   6      237  2017-11-06 16:07:47   \n",
       "4     1556   15       1  13      245  2017-11-06 16:07:47   \n",
       "5    67467   15       1  37      245  2017-11-06 16:07:47   \n",
       "6    20266   64       1  19      459  2017-11-06 16:07:47   \n",
       "7    31564   15       1  19      265  2017-11-06 16:07:47   \n",
       "8     1732    9       1  13      134  2017-11-06 16:07:47   \n",
       "9   111114   15       1  13      245  2017-11-06 16:07:47   \n",
       "10  121472    9       1   8      215  2017-11-06 16:07:47   \n",
       "11   95559    6       1  35      125  2017-11-06 16:07:47   \n",
       "12  111182   19       7  21      213  2017-11-06 16:07:47   \n",
       "13   92431   11       1  13      319  2017-11-06 16:07:47   \n",
       "14   68550   26       1  19      121  2017-11-06 16:07:47   \n",
       "15   38692   12       1  17      328  2017-11-06 16:07:47   \n",
       "16  186002    3       1  19      280  2017-11-06 16:07:47   \n",
       "17   61664    9       1  13      134  2017-11-06 16:07:47   \n",
       "18   24700   15       2  22        3  2017-11-06 16:07:47   \n",
       "19   77943   12       1  32      265  2017-11-06 16:07:47   \n",
       "\n",
       "        attributed_time  is_attributed  \n",
       "0                   NaN              0  \n",
       "1                   NaN              0  \n",
       "2                   NaN              0  \n",
       "3                   NaN              0  \n",
       "4                   NaN              0  \n",
       "5                   NaN              0  \n",
       "6                   NaN              0  \n",
       "7                   NaN              0  \n",
       "8                   NaN              0  \n",
       "9                   NaN              0  \n",
       "10                  NaN              0  \n",
       "11                  NaN              0  \n",
       "12  2017-11-06 17:29:34              1  \n",
       "13                  NaN              0  \n",
       "14                  NaN              0  \n",
       "15                  NaN              0  \n",
       "16                  NaN              0  \n",
       "17                  NaN              0  \n",
       "18                  NaN              0  \n",
       "19                  NaN              0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199644\n",
       "1       356\n",
       "Name: is_attributed, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"is_attributed\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.loc[:,[\"ip\",\"app\",\"device\",\"os\",\"channel\"]]\n",
    "X_test = df_test.loc[:,[\"ip\",\"app\",\"device\",\"os\",\"channel\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=X_train.values\n",
    "X_test= X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([83230,     3,     1,    13,   379])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([115115,      8,      1,     13,    145])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train[\"is_attributed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test =df_test[\"is_attributed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test= y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_A(n=55,i=5,o=2):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 5, output 2\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='sigmoid', input_shape=(i,)))\n",
    "    net.add(Dense(2, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn=shallow_net_A()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 55)                330       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 112       \n",
      "=================================================================\n",
      "Total params: 442\n",
      "Trainable params: 442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([83230,     3,     1,    13,   379])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 200000 samples\n",
      "Epoch 1/99\n",
      "200000/200000 [==============================] - 3s 13us/step - loss: 0.0034 - acc: 0.9982 - val_loss: 0.0020 - val_acc: 0.9983\n",
      "Epoch 2/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0019 - acc: 0.9983 - val_loss: 0.0019 - val_acc: 0.9983\n",
      "Epoch 3/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 4/99\n",
      "200000/200000 [==============================] - 3s 13us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 5/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 6/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 7/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 8/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 9/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 10/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 11/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 12/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 13/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 14/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 15/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 16/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 17/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0018 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9983\n",
      "Epoch 18/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 19/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 20/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 21/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 22/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 23/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 24/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 25/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 26/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 27/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 28/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 29/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 30/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 31/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 32/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 33/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 34/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 35/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 36/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 37/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 38/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 39/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 40/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 41/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 42/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 43/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 44/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 45/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 46/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 47/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 48/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 49/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 50/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 51/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 52/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 53/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 54/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 55/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 56/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 57/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 58/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 59/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 60/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 61/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 62/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 63/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 64/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 65/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 66/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 67/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 68/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 69/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 70/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 71/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 72/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 73/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 74/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 75/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 76/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 77/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 78/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 79/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 80/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 81/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 82/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 83/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 84/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 85/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 86/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 87/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 88/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 89/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 90/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 91/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 92/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 93/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 94/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 95/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 96/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 97/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 98/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n",
      "Epoch 99/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0017 - val_acc: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x108618a90>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 2s 10us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.001778513824111069, 0.99822]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 99.83% accuracy after 99 epochs\n",
    "nn.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 200000 samples\n",
      "Epoch 1/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 2/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 3/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 4/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 5/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 6/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 7/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 8/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 9/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 10/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 11/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 12/99\n",
      "200000/200000 [==============================] - 2s 9us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 13/99\n",
      "200000/200000 [==============================] - 2s 10us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 14/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 15/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 16/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 17/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 18/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 19/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 20/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 21/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 22/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 23/99\n",
      "200000/200000 [==============================] - 3s 13us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 24/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 25/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 26/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 27/99\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 28/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 29/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 30/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 31/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 32/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 33/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 34/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 35/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 36/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 37/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 38/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 39/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 40/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 41/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 42/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 43/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 44/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 45/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 46/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 47/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 48/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 49/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 50/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 51/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 52/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 53/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 54/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 55/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 56/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 57/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 58/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 59/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 60/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 61/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 62/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 63/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 64/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 65/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 66/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 67/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 68/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 69/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 70/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 71/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 72/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 73/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 74/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 75/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 76/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 77/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 78/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 79/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 80/99\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 81/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 82/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 83/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 84/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 85/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 86/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 87/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 88/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 89/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 90/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 91/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 92/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 93/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 94/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 95/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 96/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 97/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 98/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n",
      "Epoch 99/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0017 - acc: 0.9983 - val_loss: 0.0018 - val_acc: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x108603cf8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 99.82% accuracy after another 99 epochs\n",
    "nn.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_B(n=55,i=5,o=2):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 5, output 2\n",
    "    # Using relu\n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='relu', input_shape=(i,)))\n",
    "    net.add(Dense(2, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 55)                330       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 112       \n",
      "=================================================================\n",
      "Total params: 442\n",
      "Trainable params: 442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn2=shallow_net_B()\n",
    "nn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 200000 samples\n",
      "Epoch 1/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 2/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 3/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 4/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 5/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 6/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 7/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 8/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 9/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 10/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 11/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 12/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 13/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 14/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 15/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 16/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 17/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 18/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 19/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 20/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 21/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 22/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 23/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 24/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 25/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 26/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 27/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 28/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 29/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 30/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 31/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 32/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 33/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 34/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 35/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 36/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 37/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 38/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 39/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 40/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 41/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 42/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 43/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 44/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 45/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 46/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 47/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 48/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 49/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 50/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 51/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 52/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 53/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 54/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 55/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 56/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 57/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 58/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 59/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 60/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 61/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 62/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 63/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 64/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 65/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 66/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 67/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 68/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 69/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 70/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 71/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 72/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 73/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 74/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 75/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 76/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 77/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 78/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 79/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 80/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 81/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 82/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 83/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 84/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 85/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 86/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 87/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 88/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 89/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 90/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 91/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 92/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 93/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 94/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 95/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 96/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 97/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 98/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 99/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1b24b4e0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 3s 15us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0017778565070444528, 0.99822]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 99.82% accuracy after another 99 epochs\n",
    "nn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 200000 samples\n",
      "Epoch 1/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 2/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 3/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 4/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 5/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 6/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 7/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 8/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 9/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 10/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 11/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 12/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 13/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 14/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 15/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 16/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 17/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 18/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 19/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 20/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 21/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 22/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 23/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 24/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 25/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 26/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 27/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 28/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 29/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 30/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 31/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 32/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 33/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 34/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 35/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 36/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 37/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 38/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 39/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 40/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 41/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 42/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 43/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 44/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 45/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 46/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 47/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 48/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 49/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 50/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 51/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 52/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 53/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 54/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 55/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 56/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 57/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 58/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 59/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 60/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 61/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 62/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 63/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 64/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 65/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 66/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 67/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 68/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 69/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 70/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 71/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 72/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 73/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 74/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 75/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 76/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 77/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 78/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 79/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 80/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 81/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 82/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 83/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 84/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 85/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 86/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 87/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 88/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 89/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 90/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 91/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 92/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 93/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 94/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 95/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 96/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 97/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 98/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n",
      "Epoch 99/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.9983 - acc: 0.0017 - val_loss: 0.9982 - val_acc: 0.0018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1b2cbeb8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 3s 16us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.99822, 0.00178]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 99.82% accuracy after another 99 epochs with Relu\n",
    "# Seems to be a plateau\n",
    "nn2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shallow_net_C(n=55,i=5,o=2):\n",
    "    # create simple one dense layer net\n",
    "    # default 55 neurons, input 5, output 2\n",
    "    # Using relu and \n",
    "    net = Sequential()\n",
    "    net.add(Dense(n, activation='relu', input_shape=(i,)))\n",
    "    net.add(Dense(2, activation='softmax'))\n",
    "    # Compile net\n",
    "    net.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 55)                330       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 112       \n",
      "=================================================================\n",
      "Total params: 442\n",
      "Trainable params: 442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn3=shallow_net_C()\n",
    "nn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 200000 samples\n",
      "Epoch 1/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.5723 - acc: 0.9645 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 2/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 3/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 4/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 5/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 6/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 7/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 8/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 9/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 10/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 11/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 12/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 13/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 14/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 15/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 16/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 17/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 18/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 19/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 20/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 21/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 22/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 23/99\n",
      "200000/200000 [==============================] - 4s 21us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 24/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 25/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 26/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 27/99\n",
      "200000/200000 [==============================] - 4s 22us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 28/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 29/99\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 30/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 31/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 32/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 33/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 34/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 35/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 36/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 37/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 38/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 39/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 40/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 41/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 42/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 43/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 44/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 45/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 46/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 47/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 48/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 49/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 50/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 51/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 52/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 53/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 54/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 55/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 56/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 57/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 58/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 59/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 60/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 61/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 62/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 63/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 64/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 65/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 66/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 67/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 68/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 69/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 70/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 71/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 72/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 73/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 74/99\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 75/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 76/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 77/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 78/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 79/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 80/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 81/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 82/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 83/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 84/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 85/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 86/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 87/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 88/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 89/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 90/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 91/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 92/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 93/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 94/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 95/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 96/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 97/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 98/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 99/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1b2d1208>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 3s 17us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.028690325632095337, 0.99822]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 99.81% accuracy after first 99 epochs with Relu and Cross-entropy\n",
    "nn3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 200000 samples\n",
      "Epoch 1/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 2/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 3/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 4/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 5/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 6/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 7/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 8/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 9/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 10/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 11/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 12/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 13/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 14/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 15/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 16/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 17/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 18/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 19/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 20/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 21/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 22/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 23/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 24/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 25/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 26/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 27/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 28/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 29/99\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 30/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 31/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 32/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 33/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 34/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 35/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 36/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 37/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 38/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 39/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 40/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 41/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 42/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 43/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 44/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 45/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 46/99\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 47/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 48/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 49/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 50/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 51/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 52/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 53/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 54/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 55/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 56/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 57/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 58/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 59/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 60/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 61/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 62/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 63/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 64/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 65/99\n",
      "200000/200000 [==============================] - 4s 20us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 66/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 67/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 68/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 69/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 70/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 71/99\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 72/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 73/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 74/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 75/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 76/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 77/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 78/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 79/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 80/99\n",
      "200000/200000 [==============================] - 4s 19us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 81/99\n",
      "200000/200000 [==============================] - 4s 18us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 82/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 83/99\n",
      "200000/200000 [==============================] - 2s 11us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 84/99\n",
      "200000/200000 [==============================] - 3s 16us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 85/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 86/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 87/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 88/99\n",
      "200000/200000 [==============================] - 3s 17us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 89/99\n",
      "200000/200000 [==============================] - 3s 15us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 90/99\n",
      "200000/200000 [==============================] - 3s 14us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 91/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 92/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 93/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 94/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 95/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 96/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 97/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 98/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n",
      "Epoch 99/99\n",
      "200000/200000 [==============================] - 2s 12us/step - loss: 0.0280 - acc: 0.9983 - val_loss: 0.0287 - val_acc: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1b2ccbe0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3.fit(X_train, y_train, batch_size=128, epochs=99, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 2s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.028690325632095337, 0.99822]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 99.81% accuracy after first 99 epochs with Relu and Cross-entropy\n",
    "nn3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a fairly simple shallow net we've done fairly well classifying (99.82% accuracy after another 99 epochs with Shallow Neural Network, and 99.81% accuracy with Relu and Cross-entropy) on the TalkingData whether is-attributed classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which hyper-parameters are important?\n",
    "Input, output, bactch-size, epochs, verbose are important.\n",
    "In addition, CNN: number of layers, number of units at each layer, ReLu,and Cross-entropy\n",
    "are important as well.\n",
    "* What hyper-parameter values work best? \n",
    "In Shallow neural network, 55 neurons, input:5, output:2, batch_size: 128, epochs:99, verbose:1\n",
    "work best. \n",
    "* How the neural network compare to the supervised learners in part A?\n",
    "Neural network is proficient to give the better classification by sing non linear boundaries. In addition it is even easy to overcome overfitting by some regularizer settings. In addition, there are lots of things ML stuffs that NN might be a solution. \n",
    "However, it takes long time to be trained well relative to other methods like Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Stacked Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.95 (+/- 0.00) [KNN]\n",
      "Accuracy: 0.95 (+/- 0.02) [Random Forest]\n",
      "Accuracy: 0.93 (+/- 0.04) [Naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.00) [SVCStackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "rf = RandomForestClassifier(random_state=6)\n",
    "gau = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "sckf = StackingClassifier(classifiers=[knn, rf, gau], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "X = csv_file[['ip', 'app', 'os', 'channel']][:200]\n",
    "y = csv_file[['device']].values.ravel()[:200]\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([knn, rf, gau, sckf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'SVC'\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=10)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAHiCAYAAADMP0mlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xu8pXVd9//XO4aDCclBm0bgFkrU\n+HUnGiKkt5Xk2YJ+GTdkOhp3U+apu3wUZrehHW7t99NU8jSJOXcpQqINmSca0X5BgaMgchAZCQMc\nGFFHEDyhn98f13fjYtx79tr7u9bee8bX8/FYj32t67Cuz772Xp/He32va62VqkKSJEmL8wPLXYAk\nSdKuzDAlSZLUwTAlSZLUwTAlSZLUwTAlSZLUwTAlSZLUwTAlSdotJfnZJDcudx3a/RmmtChJrk/y\n8yP3T07y5SQ/k6SSvG+H9f8uyelt+mfbOm/YYZ1/TfKspahf0vJoveNrSb6a5OYkb0uy73LX1av1\ntDva7/XVJNuXeP8Gx2VkmFK3JGuB1wNPAT7XZj8yyU/vZLM7gGckOWy61UlagX6hqvYFjgIeBrx4\nmeuZlIdW1b7ttv9CN06yahpFafoMU+qS5DeBVwFPqKqLRhb9BfBnO9l0O/A24I+nV52klayqbgY+\nyBCqAEjylCSXJrktyQ0zI9pt2WFtBGhtkv9McmuSl4wsv1cb6fpykquAR4zuL8mPJ/lIku1Jrkzy\niyPL3pbkDUne30aWLkzyI0le0x7v00ketpjfM8lvJNmS5EtJzkty/5FlleS5Sa4Frm3zHpLk/Lb+\nNUlOGln/yUmuSnJ7kpuSvCjJvYH3A/cfGRm7//cUoqkxTKnHc4CXA8dX1eYdlr0BeNDoqcBZ/Bnw\ny0kePK0CJa1cSQ4BngRsGZl9B/BMYH+G0e7nJDlxh00fDTwYOB54aZIfb/P/GPixdnsCsHZkX3sC\n/wh8CPhh4PnA23foPycBfwTcF/gG8G/AJ9r9dwGvXsTv+Fjgf7fHXsMwev/OHVY7EXgkcGQLRucD\n72h1ngy8IcmRbd0zgd+sqv2AnwA+XFV3MBzHz4+MjH1+obVq8QxT6vE44N+BT82y7GsMYelP59q4\nvSp9E0Mgk/T94x+S3A7cAGxjZIS6qj5SVZ+qqu9U1eXAWcDP7LD9y6rqa1X1SeCTwEPb/JOAP6uq\nL1XVDcDrRrY5FtgXeEVVfbOqPgy8FzhlZJ33VNXHq+rrwHuAr1fV/6mqbwNnM5yS3JlPtFGv7Ulm\n9v104K1V9Ymq+gbDKc3jdrjE4X+3mr8GPBW4vqr+pqruqqpLgXOBX2nrfoshdP1QVX25qj4xT01a\nAoYp9XgO8CDgLUkyy/K3AKuT/MJOHuOVwBOSPHQn60javZzYRlZ+FngIw8gPAEkemeSCJF9I8hXg\nt0aXNzePTN/JEJIA7s8Q0GZ8bmT6/sANVfWdHZYfPHL/lpHpr81yf74L5R9eVfu32wtG9nt3HVX1\nVeCLO+x3tOYHMFxzOhPKtjMEsh9py38ZeDLwuSQfTXLcPDVpCRim1OMWhmH2/8ZwWu8equqbwMuA\nPwFmC1tU1ReB17R1JH0fqaqPMlw7+f+OzH4HcB5waFXdh2H0etb+MYutwKEj9//LyPTngUOT/MAO\ny29aYNkL9XmGgARAO4130A77rZHpG4CPjoSy/dtpu+cAVNXHquoEhlOA/wCcM8tjaIkZptSlnZc/\nHnhikr+cZZW/BfYBnriTh3k18NPAj+9kHUm7p9cAjxsZnd4P+FJVfT3JMcCvLuCxzgFenOSAdj3W\n80eWXcwwivX7SfZM8rPAL/C91y9N2lnAs5MclWRv4M+Bi6vq+jnWfy/D9abPaHXumeQR7eL5vZI8\nPcl9qupbwG3AzEjbLcBBSe4z5d9HszBMqVtV/SfwWOBpDBdaji77NvBS4MCdbH8bw7v/5lxH0u6p\nqr4A/B+GPgHw28DL2zVVL+W7Iy/jeBnDKbX/YLjQ/G9H9vNNhvD0JOBWhtH0Z1bVp3t/h52pqn8G\n/hfDdU9bGS6OP3kn698OPL6t83mGU5qvBPZuqzwDuD7JbQynQJ/etvs0Q3C7rp0e9N18SyhVjgxK\nkiQtliNTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHQxTkiRJHZb2G6ovOsO3DkrfT376+eN+2OLKd9lZ\nxZ23LncVkpbKDx4ER/3qWD3MkSlJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJ\nkqQOhilJkqQOhilJkqQOhilJkqQOY4WpJPsneVeSTye5OslxSQ5Mcn6Sa9vPA6ZdrCQtlP1L0rSN\nOzL1WuADVfUQ4KHA1cBpwKaqOgLY1O5L0kpj/5I0VfOGqST3AR4DnAlQVd+squ3ACcCGttoG4MRp\nFSlJi2H/krQUxhmZOhz4AvA3SS5N8pYk9wZWV9XWts7NwOppFSlJi2T/kjR144SpVcDDgTdW1cOA\nO9hhSLyqCqjZNk6yLsnmJJvXb7ywt15JWojJ9a9zN029WEm7pnHC1I3AjVV1cbv/LobmdEuSNQDt\n57bZNq6q9VV1dFUdve6ER02iZkka1+T61y8fvyQFS9r1zBumqupm4IYkD26zjgeuAs4D1rZ5a4GN\nU6lQkhbJ/iVpKawac73nA29PshdwHfBshiB2TpJTgc8BJ02nREnqYv+SNFVjhamqugw4epZFjntL\nWtHsX5KmzU9AlyRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCY\nkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ\n6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6rBqnJWSXA/cDnwbuKuqjk5yIHA2cBhwPXBSVX15OmVK\n0uLYvyRN20JGpn6uqo6qqqPb/dOATVV1BLCp3Zeklcj+JWlqek7znQBsaNMbgBP7y5GkJWH/kjQx\n44apAj6U5ONJ1rV5q6tqa5u+GVg98eokqZ/9S9JUjRumHl1VDweeBDw3yWNGF1ZVMTSs75FkXZLN\nSTav33hhX7WStHCT6V/nblqCUiXtisa6AL2qbmo/tyV5D3AMcEuSNVW1NckaYNsc264H1gNw0Rmz\nNixJmpaJ9a/LziruvHWJqpa0K5l3ZCrJvZPsNzMNPB64AjgPWNtWWwtsnFaRkrQY9i9JS2GckanV\nwHuSzKz/jqr6QJKPAeckORX4HHDS9MqUpEWxf0maunnDVFVdBzx0lvlfBI6fRlGSNAn2L0lLwU9A\nlyRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ\n6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCY\nkiRJ6mCYkiRJ6jB2mEqyR5JLk7y33T88ycVJtiQ5O8le0ytTkhbP/iVpmhYyMvVC4OqR+68E/rKq\nHgh8GTh1koVJ0gTZvyRNzVhhKskhwFOAt7T7AR4LvKutsgE4cRoFSlIP+5ekaRt3ZOo1wO8D32n3\nDwK2V9Vd7f6NwMETrk2SJsH+JWmq5g1TSZ4KbKuqjy9mB0nWJdmcZPP6jRcu5iEkaVEm2r/O3TTh\n6iTtLlaNsc6jgF9M8mRgH+CHgNcC+ydZ1V7dHQLcNNvGVbUeWA/ARWfUJIqWpDFNrn9ddlZx561L\nUrSkXcu8I1NV9eKqOqSqDgNOBj5cVU8HLgCe1lZbC2ycWpWStAj2L0lLoedzpv4A+N0kWxiuQThz\nMiVJ0tTZvyRNzDin+e5WVR8BPtKmrwOOmXxJkjR59i9J0+InoEuSJHUwTEmSJHUwTEmSJHUwTEmS\nJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUw\nTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHUwTEmSJHWYN0wl\n2SfJJUk+meTKJC9r8w9PcnGSLUnOTrLX9MuVpPHZvyQthXFGpr4BPLaqHgocBTwxybHAK4G/rKoH\nAl8GTp1emZK0KPYvSVM3b5iqwVfb3T3brYDHAu9q8zcAJ06lQklaJPuXpKUw1jVTSfZIchmwDTgf\n+CywvaruaqvcCBw8x7brkmxOsnn9xgsnUbMkjW1i/evcTUtTsKRdzqpxVqqqbwNHJdkfeA/wkHF3\nUFXrgfUAXHRGLaJGSVq0ifWvy84q7rx1KjVK2rUt6N18VbUduAA4Dtg/yUwYOwS4acK1SdLE2L8k\nTcs47+a7X3tFR5J7AY8DrmZoSk9rq60FNk6rSElaDPuXpKUwzmm+NcCGJHswhK9zquq9Sa4C3pnk\nT4FLgTOnWKckLYb9S9LUzRumqupy4GGzzL8OOGYaRUnSJNi/JC0FPwFdkiSpg2FKkiSpg2FKkiSp\ng2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FK\nkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpg2FKkiSpw7xhKsmh\nSS5IclWSK5O8sM0/MMn5Sa5tPw+YfrmSND77l6SlMM7I1F3A71XVkcCxwHOTHAmcBmyqqiOATe2+\nJK0k9i9JUzdvmKqqrVX1iTZ9O3A1cDBwArChrbYBOHFaRUrSYti/JC2FBV0zleQw4GHAxcDqqtra\nFt0MrJ5oZZI0QfYvSdMydphKsi9wLvA7VXXb6LKqKqDm2G5dks1JNq/feGFXsZK0GBPpX+duWoJK\nJe2KVo2zUpI9GRrR26vq3W32LUnWVNXWJGuAbbNtW1XrgfUAXHTGrA1LkqZlYv3rsrOKO29dipIl\n7WLGeTdfgDOBq6vq1SOLzgPWtum1wMbJlydJi2f/krQUxhmZehTwDOBTSS5r8/4QeAVwTpJTgc8B\nJ02nRElaNPuXpKmbN0xV1b8CmWPx8ZMtR5Imx/4laSn4CeiSJEkdDFOSJEkdDFOSJEkdDFOSJEkd\nDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOS\nJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkdDFOSJEkd5g1TSd6aZFuSK0bm\nHZjk/CTXtp8HTLdMSVoce5ikaRtnZOptwBN3mHcasKmqjgA2tfuStBK9DXuYpCmaN0xV1b8AX9ph\n9gnAhja9AThxwnVJ0kTYwyRN22KvmVpdVVvb9M3A6gnVI0lLwR4maWK6L0CvqgJqruVJ1iXZnGTz\n+o0X9u5OkiZqZz3sHv3r3E1LXJmkXcWqRW53S5I1VbU1yRpg21wrVtV6YD0AF50xZ+iSpCU0Vg+7\nR/+67KzizluXsERJu4rFjkydB6xt02uBjZMpR5KWhD1M0sSM89EIZwH/Bjw4yY1JTgVeATwuybXA\nz7f7krTi2MMkTdu8p/mq6pQ5Fh0/4VokaeLsYZKmzU9AlyRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCY\nkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ\n6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6mCYkiRJ6tAVppI8Mck1\nSbYkOW1SRUnSUrCHSZqERYepJHsArweeBBwJnJLkyEkVJknTZA+TNCk9I1PHAFuq6rqq+ibwTuCE\nyZQlSVNnD5M0Eas6tj0YuGHk/o3AI3e6xer/q2N3kjRRC+th9z0CvrF62jVJWin22m/sVXvC1FiS\nrAPWtbu/WVXrp73PcSVZt5LqmWFdC7MS61qJNcHKrWulsn8tnHUtjHUtzEqtq+c0303AoSP3D2nz\n7qGq1lfV0e220g7AuvlXWRbWtTArsa6VWBOs3LqWw7w9zP61KNa1MNa1MCuyrp4w9THgiCSHJ9kL\nOBk4bzJlSdLU2cMkTcSiT/NV1V1Jngd8ENgDeGtVXTmxyiRpiuxhkial65qpqnof8L4J1bIcVtqw\n/QzrWpiVWNdKrAlWbl3LYhfvYSv1b2ldC2NdC7Mi60pVLXcNkiRJuyy/TkaSJKlHVe3yN+B64FPA\nZcDmNu9A4Hzg2vbzgDY/wOuALcDlwMNHHmdtW/9aYO3I/J9qj7+lbZs56ngrsA24YmTe1OuYax/z\n1HU6wzuXLmu3J48se3HbxzXAE0bmP7HN2wKcNjL/cODiNv9sYK82f+92f0tbftjINocCFwBXAVcC\nL1wJx2sndS338doHuAT4ZKvrZR2PNZF6vdm/JlHHXPuYp67TWcbnY1tuD7OHfff/YbkbyUR+iaEZ\n3XeHeX8xczCB04BXtuknA+9v/9jHAheP/HNe134e0KZnngSXtHXTtn3SHHU8Bng493zST72OufYx\nT12nAy+a5Xc4sv2z793+AT/LcHHuHm36R4G92jpHtm3OAU5u028CntOmfxt4U5s+GTh7ZD9raM0E\n2A/4TNv3sh6vndS13McrwL5tek+GxnDsQh9rkvV6s39Noo659jFPXaezjM/HNs8etoBjxm7ew5a9\nkUzkl5i9GV0DrBn557qmTb8ZOGXH9YBTgDePzH9zm7cG+PTI/HusN0sth3HPJ/3U65hrH/PUNdcT\n68XAi0fufxA4rt0+uON67QlyK7Cqzb97vZlt2/Sqtt5cr4o3Ao9bKcdrlrpWzPECfhD4BMOndS/o\nsSZZrzf71yTqGOf5OEtdK+b5OPKY9rDv4x62u1wzVcCHkny8fWIxwOqq2tqmbwZmvgditq+QOHie\n+TfOMn9cS1HHXPuYz/OSXJ7krUkOWGRdBwHbq+quWeq6e5u2/Ctt/XtIchjwMIZXKivmeO1QFyzz\n8UqyR5LLGE55nM/wKmyhjzXJejUZ9q9duH+BPYwxj9nu3MN2lzD16Kp6OMO3vz83yWNGF9YQR2tZ\nKlviOhawjzcCPwYcBWwFXjXNuuaSZF/gXOB3quq20WXLebxmqWvZj1dVfbuqjmL4pO5jgIcsdQ2a\nCvvXwvex7M/HGfaw8e3OPWy3CFNVdVP7uQ14D8Mf6ZYkawDaz21t9bm+QmJn8w+ZZf64lqKOufYx\np6q6pf1jfwf4a4Zjtpi6vgjsn2TVDvPv8Vht+X3a+rR5ezI82d9eVe+e53dZsuM1W10r4XjNqKrt\nDBeYHreIx5pkvZoA+9eu2b/afHuYPQzYDcJUknsn2W9mGng8cAXD10KsbautZThvTJv/zAyOBb7S\nhks/CDw+yQFt+PPxDOdVtwK3JTk2SYBnjjzWOJaijrn2MaeZJ2LzSwzHbOaxTk6yd5LDgSMYLoKc\n9as32quiC4CnzfE7ztT1NODDbX3a73AmcHVVvXqlHK+56loBx+t+SfZv0/diuAbi6kU81iTrVSf7\n1073Maflfj62GuxhCzhmu30PW+hFVivtxnDl/if57tstX9LmHwRsYnjr6D8DB878TYHXM5yr/RRw\n9Mhj/TrDWye3AM8emX80wz/eZ4G/Yu6LqM9iGD79FsN52VOXoo659jFPXX/b9ns5wz/nmpH1X9L2\ncQ0j7/xheDfKZ9qyl+zwN7ik1fv3wN5t/j7t/pa2/EdHtnk0w9D05Yy8VXe5j9dO6lru4/WTwKVt\n/1cAL+14rInU683+NYk65trHPHUt6/OxLbeH2cPuvvkJ6JIkSR12+dN8kiRJy8kwJUmS1MEwJUmS\n1MEwJUmS1MEwJUmS1MEwJUmS1MEwJUmS1MEwpe+R5A+TvGW565CkuSR5W5I/nWPZm5L8r+WuY0KP\n/9UkP9qm75XkH5N8JcnfJ3l6kg9Na98an2FqN5Tk+iTb2tdTzMz7H0k+Ms72VfXnVfU/plDXR5J8\nvTWHryT5lyT/ddL7kbS8kjw6yUXtef6lJBcmeUSSZyX512nvv6p+q6r+ZBKP1b7+5QVJrkhyR5Ib\nW5BZkt5VVftW1XXt7tOA1cBBVfUrVfX2qnr8UtShnTNM7b72AF643EXM4nlVtS9wIPARhq84kLSb\nSPJDwHuBMxie5wcDLwO+sZx1dXgtQy99AcPv8yDgH4CnLEMtDwA+U1V39T5Qkj0mUI8aw9Tu6/8B\nXjTzxZI7SvLaJDckuS3Jx5P8t5Flpyf5uzb9/iTP22HbTyb5v9v0Q5Kc3159XpPkpHGKq6pvA+8E\njhx53GOS/FuS7Um2Jvmr9oWVJHl9klftUMd5Sf5nm75/knOTfCHJfyR5wQ6Pu7n9rrckGf1SUkmT\n9SCAqjqrqr5dVV+rqg8xfLfem4Dj2uj0doAkT0lyaXt+3pDk9NEHGxnl2t6WP2vHHSbZL8kFSV7X\nRpLuPvWW5GfbaNLvtRH7rUmePbLtQe3U2W1JPpbkT2dGz5IcATwXOKWqPlxV36iqO9uI0CtmqeOA\nJO9tfejLbfqQkeXPSnJdkttbn3p6m//AJB9tI3m3Jjl7ZJtqy18GvBT47+34nbrjSN/O+nE7Jm9M\n8r4kdwA/N/6fVPMxTO2+NjOM/LxojuUfA45ieKX1DuDvk+wzy3pnAafM3ElyJMOro3/KcBrx/Lb9\nDzN8S/cb2jo71ULS04F/H5n9beB/AvcFjgOOB367LdsAnJLkB9r29wV+HnhHm/ePDF8We3Db7neS\nPKFt+1rgtVX1Q8CPAefMV5+kRfsM8O0kG5I8KckBAFV1NfBbwL+1U1czL/TuAJ4J7M8w2vOcJCcC\nJHkA8H6GUa77MfSsy0Z3lmTmS38vrKoX1OxfOPsjwH0Y+sOpwOtn6mL48uE72jpr223G8cCNVXXJ\nmL/7DwB/w9Aj/wvwNYYvKKb1y9cxfDHvfsBPj/wufwJ8CDgAOKT9vvdQVX8M/Dlwdjt+Z+5wHMbp\nx78K/BmwHzD1063fTwxTu7eXAs9Pcr8dF1TV31XVF6vqrqp6FbA38OBZHuM9wFGtqcEQgN5dVd8A\nngpcX1V/0x7nUuBc4Fd2UtPr2ivS24HnMQz/z9T08ar69/ZY1wNvBn6mLbsE+ApDc4OhUXykqm4B\nHgHcr6peXlXfbNcX/HVbB4ZXxA9Mct+q+mpVjQY4SRNUVbcBjwaK4Xn4hTaKvHqO9T9SVZ+qqu9U\n1eUML+B+pi3+VeCf2yjXt1rPGg1T9wc+Cvx9Vf3RTsr6FvDy9hjvA74KPDjDqa5fBv64jThdxfDC\nbcZBwNYF/O5frKpz22PdzhBcfmZkle8AP5HkXlW1taquHKnvAcD9q+rrVbWYoDNOP95YVRe2Y/31\nRexDczBM7caq6gqGaxdO23FZkhcluboNK29neNV231ke43bgn/huMDkFeHubfgDwyDb8vr09ztMZ\nXuHN5QXtFem9GJ7870ryk62mB7Vh8ZuT3MbwKmy0pg3Ar7XpX+O711s9ALj/DnX8IcOFmjC8En0Q\n8Ok2jP/UndQnqVNVXV1Vz6qqQ4CfYAg9r5lt3SSPbKfovpDkKwyjVzPP+0OBz+5kV09h6CVvmqek\nL+5wndGdwL4Mo12rgBtGlo1OfxFYM89j3y3JDyZ5c5LPtR72L8D+SfaoqjuA/87w+21N8k9JHtI2\n/X0gwCVJrkzy6+Puc8Q4/fiG2TdVL8PU7u+Pgd9gGN4GIMP1Ub8PnAQc0MLNVxiezLM5i+EU23HA\nPsAFbf4NwEerav+R275V9Zz5imqvjP4/YAsw826UNwKfBo5op+T+cIea/g44IclDgR9nuAh0po7/\n2KGO/arqyW1f11bVKQxD369kCHD3RtLUVdWngbcxhKrZTsG9AzgPOLSq7sMQjGae9zcwnJqfy18D\nHwDet8jn9BeAuxhOrc04dGR6E3BIkqPHfLzfYxjhf2TrYY9p8wNQVR+sqscxBLRPt/qpqpur6jeq\n6v7AbzKcnnvgAn+XcfrxbMdfE2CY2s1V1RbgbIZ3oszYj6GBfAFYleSlwA/t5GHex/Cq5+UM5+u/\n0+a/F3hQkmck2bPdHpHkx8eprYWzI4GZoe79gNuAr7ZXbPcIZVV1I8O1Xn8LnFtVX2uLLgFuT/IH\nGT6HZY8kP5HkEW0/v5bkfq3u7W2b7yBp4tpF0L83c+F1kkMZRrT/HbiFIZzsNbLJfsCXqurrSY5h\nOLU34+3Azyc5KcmqDBeLH7XDLp8HXAP8Y5J7LaTW9kaYdwOnt1GlhzBcvzWz/FrgDcBZGS5k3yvJ\nPklOTvI9I/7td/kasD3JgQwvZmeOy+okJ7TQ9w2GU43fact+Jd+9UP3LDKFnoT2qqx+rj2Hq+8PL\ngdFXbR9keDX3GeBzwNfZyfBvuz7q3bQLvkfm384wqnQy8HngZoaRn713UstftXeifJUhFP1RVb2/\nLXsRQyO9neEV29mzbL8B+K+MfKRCa4hPZbg49T+AW4G3MJy6BHgicGXb52uBk0eCmKTJuh14JHBx\ne9fYvwNXMIzafJjhxdPNSW5t6/828PIktzNc53n3G0Sq6j+BJ7dtv8RwwfZDR3fWLjhfB9wIbJzj\njTQ78zyGXnEzQ185i3t+jMMLGC4ifz3Di7HPAr/E8KaXHb2G4bTjre33/sDIsh8AfpehV36J4Vqq\nmReMj2A4Xl9lGKV7YX33s6XGssh+rAnJ7G98kFamJI9hON33gDnetSNJi5bklcCPVNXaeVeWGkem\ntMtIsifDh+e9xSAlaRLaacmfzOAYhjesvGe569KuxTClXUI777+d4cLNWd8VJEmLsB/DZQx3MFxa\n8Cpg47JWpF2Op/kkSZI6ODIlSZLUwTAlSZLUYdWS7u3jb/OcovT95KeeNdcHwe56rjqv+NqXlrsK\nSUvlXgfAkSeM1cOWNkx94/Yl3Z0kTcw377CHSd9P9thr/nUaT/NJkiR1MExJkiR1MExJkiR1MExJ\nkiR1MExJkiR1MExJkiR1MExJkiR1MExJkiR1MExJkiR1MExJkiR1GCtMJdk/ybuSfDrJ1UmOS3Jg\nkvOTXNt+HjDtYiVpoexfkqaVo5T+AAAQWklEQVRt3JGp1wIfqKqHAA8FrgZOAzZV1RHApnZfklYa\n+5ekqZo3TCW5D/AY4EyAqvpmVW0HTgA2tNU2ACdOq0hJWgz7l6SlMM7I1OHAF4C/SXJpkrckuTew\nuqq2tnVuBlbPtnGSdUk2J9m8fuOFk6laksYzuf517qYlKlnSrmacMLUKeDjwxqp6GHAHOwyJV1UB\nNdvGVbW+qo6uqqPXnfCo3nolaSEm179++fipFytp1zROmLoRuLGqLm7338XQnG5Jsgag/dw2nRIl\nadHsX5Kmbt4wVVU3AzckeXCbdTxwFXAesLbNWwtsnEqFkrRI9i9JS2HVmOs9H3h7kr2A64BnMwSx\nc5KcCnwOOGk6JUpSF/uXpKkaK0xV1WXA0bMs8iICSSua/UvStPkJ6JIkSR0MU5IkSR0MU5IkSR0M\nU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5Ik\nSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR1WjbNSkuuB\n24FvA3dV1dFJDgTOBg4DrgdOqqovT6dMSVoc+5ekaVvIyNTPVdVRVXV0u38asKmqjgA2tfuStBLZ\nvyRNTc9pvhOADW16A3BifzmStCTsX5ImZtwwVcCHknw8ybo2b3VVbW3TNwOrJ16dJPWzf0maqnHD\n1KOr6uHAk4DnJnnM6MKqKoaG9T2SrEuyOcnm9Rsv7KtWkhZuMv3r3E1LUKqkXdFYF6BX1U3t57Yk\n7wGOAW5JsqaqtiZZA2ybY9v1wHoALjpj1oYlSdMysf512VnFnbcuUdWSdiXzjkwluXeS/WamgccD\nVwDnAWvbamuBjdMqUpIWw/4laSmMMzK1GnhPkpn131FVH0jyMeCcJKcCnwNOml6ZkrQo9i9JUzdv\nmKqq64CHzjL/i8Dx0yhKkibB/iVpKfgJ6JIkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5Ik\nSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0M\nU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR0MU5IkSR3GDlNJ9khyaZL3tvuHJ7k4yZYkZyfZ\na3plStLi2b8kTdNCRqZeCFw9cv+VwF9W1QOBLwOnTrIwSZog+5ekqRkrTCU5BHgK8JZ2P8BjgXe1\nVTYAJ06jQEnqYf+SNG3jjky9Bvh94Dvt/kHA9qq6q92/ETh4wrVJ0iTYvyRN1bxhKslTgW1V9fHF\n7CDJuiSbk2xev/HCxTyEJC3KRPvXuZsmXJ2k3cWqMdZ5FPCLSZ4M7AP8EPBaYP8kq9qru0OAm2bb\nuKrWA+sBuOiMmkTRkjSmyfWvy84q7rx1SYqWtGuZd2Sqql5cVYdU1WHAycCHq+rpwAXA09pqa4GN\nU6tSkhbB/iVpKfR8ztQfAL+bZAvDNQhnTqYkSZo6+5ekiRnnNN/dquojwEfa9HXAMZMvSZImz/4l\naVr8BHRJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQO\nhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJ\nkqQOhilJkqQOhilJkqQOhilJkqQO84apJPskuSTJJ5NcmeRlbf7hSS5OsiXJ2Un2mn65kjQ++5ek\npTDOyNQ3gMdW1UOBo4AnJjkWeCXwl1X1QODLwKnTK1OSFsX+JWnq5g1TNfhqu7tnuxXwWOBdbf4G\n4MSpVChJi2T/krQUxrpmKskeSS4DtgHnA58FtlfVXW2VG4GDp1OiJC2e/UvStI0Vpqrq21V1FHAI\ncAzwkHF3kGRdks1JNq/feOEiy5SkxZlY/zp309RqlLRrW7WQlatqe5ILgOOA/ZOsaq/uDgFummOb\n9cB6AC46o/rKlaTF6e5fl51V3HnrUpUraRcyzrv57pdk/zZ9L+BxwNXABcDT2mprgY3TKlKSFsP+\nJWkpjDMytQbYkGQPhvB1TlW9N8lVwDuT/ClwKXDmFOuUpMWwf0maunnDVFVdDjxslvnXMVx/IEkr\nkv1L0lLwE9AlSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYk\nSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6GKYkSZI6\nGKYkSZI6GKYkSZI6GKYkSZI6zBumkhya5IIkVyW5MskL2/wDk5yf5Nr284DplytJ47N/SVoK44xM\n3QX8XlUdCRwLPDfJkcBpwKaqOgLY1O5L0kpi/5I0dfOGqaraWlWfaNO3A1cDBwMnABvaahuAE6dV\npCQthv1L0lJY0DVTSQ4DHgZcDKyuqq1t0c3A6olWJkkTZP+SNC1jh6kk+wLnAr9TVbeNLquqAmqO\n7dYl2Zxk8/qNF3YVK0mLMZH+de6mJahU0q5o1TgrJdmToRG9vare3WbfkmRNVW1NsgbYNtu2VbUe\nWA/ARWfM2rAkaVom1r8uO6u489alKFnSLmacd/MFOBO4uqpePbLoPGBtm14LbJx8eZK0ePYvSUth\nnJGpRwHPAD6V5LI27w+BVwDnJDkV+Bxw0nRKlKRFs39Jmrp5w1RV/SuQORYfP9lyJGly7F+SloKf\ngC5JktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJ\nktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTBMCVJktTB\nMCVJktTBMCVJktTBMCVJktRh3jCV5K1JtiW5YmTegUnOT3Jt+3nAdMuUpMWxh0matnFGpt4GPHGH\neacBm6rqCGBTuy9JK9HbsIdJmqJ5w1RV/QvwpR1mnwBsaNMbgBMnXJckTYQ9TNK0LfaaqdVVtbVN\n3wysnmvFJOuSbE6yef3GCxe5O0maqLF62D3617mblq46SbuUVb0PUFWVpHayfD2wHoCLzphzPUla\nDjvrYffoX5edVdx561KWJmkXsdiRqVuSrAFoP7dNriRJmjp7mKSJWWyYOg9Y26bXAhsnU44kLQl7\nmKSJGeejEc4C/g14cJIbk5wKvAJ4XJJrgZ9v9yVpxbGHSZq2ea+ZqqpT5lh0/IRrkaSJs4dJmjY/\nAV2SJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqS\nJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmDYUqSJKmD\nYUqSJKmDYUqSJKlDV5hK8sQk1yTZkuS0SRUlSUvBHiZpEhYdppLsAbweeBJwJHBKkiMnVZgkTZM9\nTNKk9IxMHQNsqarrquqbwDuBEyZTliRNnT1M0kSs6tj2YOCGkfs3Ao/c6Rb3/uGO3UnSRC2sh/3g\ngRAvM5W+b+xzn7FX7QlTY0myDljX7v5mVa2f9j7HlWTdSqpnhnUtzEqsayXWBCu3rpXK/rVw1rUw\n1rUwK7WunpdZNwGHjtw/pM27h6paX1VHt9tKOwDr5l9lWVjXwqzEulZiTbBy61oO8/Yw+9eiWNfC\nWNfCrMi6esLUx4AjkhyeZC/gZOC8yZQlSVNnD5M0EYs+zVdVdyV5HvBBYA/grVV15cQqk6QpsodJ\nmpSua6aq6n3A+yZUy3JYacP2M6xrYVZiXSuxJli5dS2LXbyHrdS/pXUtjHUtzIqsK1W13DVIkiTt\nsnyfryRJUo+q2uVvwPXAp4DLgM1t3oHA+cC17ecBbX6A1wFbgMuBh488ztq2/rXA2pH5P9Uef0vb\nNnPU8VZgG3DFyLyp1zHXPuap63SGdy5d1m5PHln24raPa4AnjMx/Ypu3BThtZP7hwMVt/tnAXm3+\n3u3+lrb8sJFtDgUuAK4CrgReuBKO107qWu7jtQ9wCfDJVtfLOh5rIvV6s39Noo659jFPXaezjM/H\nttweZg/77v/DcjeSifwSQzO67w7z/mLmYAKnAa9s008G3t/+sY8FLh7557yu/TygTc88CS5p66Zt\n+6Q56ngM8HDu+aSfeh1z7WOeuk4HXjTL73Bk+2ffu/0Dfpbh4tw92vSPAnu1dY5s25wDnNym3wQ8\np03/NvCmNn0ycPbIftbQmgmwH/CZtu9lPV47qWu5j1eAfdv0ngyN4diFPtYk6/Vm/5pEHXPtY566\nTmcZn49tnj1sAceM3byHLXsjmcgvMXszugZYM/LPdU2bfjNwyo7rAacAbx6Z/+Y2bw3w6ZH591hv\nlloO455P+qnXMdc+5qlrrifWi4EXj9z/IHBcu31wx/XaE+RWYFWbf/d6M9u26VVtvbleFW8EHrdS\njtcsda2Y4wX8IPAJhk/rXtBjTbJeb/avSdQxzvNxlrpWzPNx5DHtYd/HPWx3uWaqgA8l+Xj7xGKA\n1VW1tU3fDKxu07N9hcTB88y/cZb541qKOubax3yel+TyJG9NcsAi6zoI2F5Vd81S193btOVfaevf\nQ5LDgIcxvFJZMcdrh7pgmY9Xkj2SXMZwyuN8hldhC32sSdarybB/7cL9C+xhjHnMducetruEqUdX\n1cMZvv39uUkeM7qwhjhay1LZEtexgH28Efgx4ChgK/CqadY1lyT7AucCv1NVt40uW87jNUtdy368\nqurbVXUUwyd1HwM8ZKlr0FTYvxa+j2V/Ps6wh41vd+5hu0WYqqqb2s9twHsY/ki3JFkD0H5ua6vP\n9RUSO5t/yCzzx7UUdcy1jzlV1S3tH/s7wF8zHLPF1PVFYP8kq3aYf4/Hasvv09anzduT4cn+9qp6\n9zy/y5Idr9nqWgnHa0ZVbWe4wPS4RTzWJOvVBNi/ds3+1ebbw+xhwG4QppLcO8l+M9PA44ErGL4W\nYm1bbS3DeWPa/GdmcCzwlTZc+kHg8UkOaMOfj2c4r7oVuC3JsUkCPHPkscaxFHXMtY85zTwRm19i\nOGYzj3Vykr2THA4cwXAR5KxfvdFeFV0APG2O33GmrqcBH27r036HM4Grq+rVK+V4zVXXCjhe90uy\nf5u+F8M1EFcv4rEmWa862b92uo85LffzsdVgD1vAMdvte9hCL7JaaTeGK/c/yXffbvmSNv8gYBPD\nW0f/GThw5m8KvJ7hXO2ngKNHHuvXGd46uQV49sj8oxn+8T4L/BVzX0R9FsPw6bcYzsueuhR1zLWP\neer627bfyxn+OdeMrP+Sto9rGHnnD8O7UT7Tlr1kh7/BJa3evwf2bvP3afe3tOU/OrLNoxmGpi9n\n5K26y328dlLXch+vnwQubfu/Anhpx2NNpF5v9q9J1DHXPuapa1mfj225PcwedvfNT0CXJEnqsMuf\n5pMkSVpOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQOhilJkqQO/z9ls3fTHboK\neQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "for clf, lab, grd in zip([knn, rf, gau, sckf], \n",
    "                         ['KNN', \n",
    "                          'Random Forest', \n",
    "                          'Naive Bayes',\n",
    "                          'StackingClassifier'],\n",
    "                          itertools.product([0, 1], repeat=2)):\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, filler_feature_values={2: 1, 3 : 1.5})\n",
    "    plt.title(lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Probabilities as Meta-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.95 (+/- 0.00) [KNN]\n",
      "Accuracy: 0.95 (+/- 0.02) [Random Forest]\n",
      "Accuracy: 0.93 (+/- 0.04) [Naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.02) [SVCStackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "rf = RandomForestClassifier(random_state=6)\n",
    "gau = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "sckf = StackingClassifier(classifiers=[knn, rf, gau], \n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=lr)\n",
    "\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([knn, rf, gau, sckf], \n",
    "                      ['KNN', \n",
    "                       'Random Forest', \n",
    "                       'Naive Bayes',\n",
    "                       'SVC'\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=10)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Stacked Classification and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.950 +/- 0.00 {'kneighborsclassifier__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1}\n",
      "0.950 +/- 0.00 {'kneighborsclassifier__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1}\n",
      "0.925 +/- 0.01 {'kneighborsclassifier__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0}\n",
      "0.950 +/- 0.00 {'kneighborsclassifier__n_neighbors': 1, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0}\n",
      "0.950 +/- 0.00 {'kneighborsclassifier__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1}\n",
      "0.950 +/- 0.00 {'kneighborsclassifier__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 0.1}\n",
      "0.950 +/- 0.01 {'kneighborsclassifier__n_neighbors': 5, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 10.0}\n",
      "0.945 +/- 0.01 {'kneighborsclassifier__n_neighbors': 5, 'randomforestclassifier__n_estimators': 50, 'meta-logisticregression__C': 10.0}\n",
      "Best parameters: {'kneighborsclassifier__n_neighbors': 1, 'randomforestclassifier__n_estimators': 10, 'meta-logisticregression__C': 0.1}\n",
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "rf = RandomForestClassifier(random_state=6)\n",
    "gau = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[knn, rf, gau], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "params = {'kneighborsclassifier__n_neighbors': [1, 5],\n",
    "          'randomforestclassifier__n_estimators': [10, 50],\n",
    "          'meta-logisticregression__C': [0.1, 10.0]}\n",
    "\n",
    "grid = GridSearchCV(estimator=sclf, \n",
    "                    param_grid=params, \n",
    "                    cv=5,\n",
    "                    refit=True)\n",
    "grid.fit(X, y)\n",
    "\n",
    "cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "\n",
    "for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "    print(\"%0.3f +/- %0.2f %r\"\n",
    "          % (grid.cv_results_[cv_keys[0]][r],\n",
    "             grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "             grid.cv_results_[cv_keys[2]][r]))\n",
    "\n",
    "print('Best parameters: %s' % grid.best_params_)\n",
    "print('Accuracy: %.2f' % grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking of Classifiers that Operate on Different Feature Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(average_probas=False,\n",
       "          classifiers=[Pipeline(memory=None,\n",
       "     steps=[('columnselector', ColumnSelector(cols=(0, 2), drop_axis=False)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          pen...='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])],\n",
       "          meta_classifier=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          store_train_meta_features=False, use_clones=True,\n",
       "          use_features_in_secondary=False, use_probas=False, verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1 = make_pipeline(ColumnSelector(cols=(0, 2)),\n",
    "                      LogisticRegression())\n",
    "pipe2 = make_pipeline(ColumnSelector(cols=(1, 2, 3)),\n",
    "                      LogisticRegression())\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[pipe1, pipe2], \n",
    "                          meta_classifier=LogisticRegression())\n",
    "\n",
    "sclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did the stacked ensemble super-model help?\n",
    "    Yes. The ensemble methods successfully boost predictive accuracy by combining the predictions of multiple machine learning models. As an efficient ensemble method, its predictions, generated by using various machine learning algorithms, are used as inputs in a second-layer learning algorithm. This second-layer algorithm is trained to optimally combine the model predictions to form a new set of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer the following questions for the stacked ensemble super-model:\n",
    "1. How did to combine the models?\n",
    "       Stacking combine multiple classification models via a meta-classifier as an ensemble learning technique. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n",
    "       As an efficient ensemble method, its predictions, generated by using various machine learning algorithms, are used as inputs in a second-layer learning algorithm. This second-layer algorithm is trained to optimally combine the model predictions to form a new set of predictions.\n",
    "2. Cross-validate the model. How well did it do?\n",
    "       All observations are used for both training and validation, and each observation is used for validation exactly once. In repeated cross-validation, the cross-validation procedure is repeated n times, yielding n random partitions of the original sample. The n results are again averaged (or otherwise combined) to produce a single estimation. It tells you generically how good the algorithm you chose will be at predicting out-of-sample. And in here, it successfully proves that the stacked ensemble super-model works well and this model really boost predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
